{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qEbW4MaEfXNNHXztFOb6R5X--7rTx5mF","authorship_tag":"ABX9TyPqvRlzZ6chh4/aFf/+yykq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","# Deep Learning Approach at \n","\n"],"metadata":{"id":"95C3yQodW1rD"}},{"cell_type":"markdown","source":["## Importing Code and Drive"],"metadata":{"id":"loRDwnkAau5j"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"1GxtiZoHVY37","executionInfo":{"status":"ok","timestamp":1669753034695,"user_tz":300,"elapsed":16342,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"d2453e9c-991e-4746-e9af-ffbf58f788e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd \n","import torch.nn.functional as F\n","from torchvision.datasets.utils import download_url\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import random_split\n","import torchvision.transforms as tt\n","from torch.utils.data import random_split\n","from torchvision.utils import make_grid\n","from torchvision import datasets\n","from torchvision import models\n","import nltk\n","import gensim\n","from gensim.models import Word2Vec\n"],"metadata":{"id":"1-3YzUSYWTGO","executionInfo":{"status":"ok","timestamp":1669753043053,"user_tz":300,"elapsed":4815,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Importing Data\n","\n","data was located in a csv file. For this project, only the lyrics will be used to find a relation with the binary valence variable denoting if a song is happy or sad."],"metadata":{"id":"kwVjiCF9W_G_"}},{"cell_type":"code","source":["df = pd.read_csv('./drive/MyDrive/NLPproject/data_bin.csv')\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":965},"id":"z0-pE9t8WqjX","executionInfo":{"status":"ok","timestamp":1669753043447,"user_tz":300,"elapsed":410,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"9b6fb945-716d-4a1f-d14f-6ddd9a813a39"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0  Unnamed: 0.1  \\\n","0              0          2949   \n","1              1         65958   \n","2              2         43234   \n","3              3          5766   \n","4              4         61410   \n","...          ...           ...   \n","5995        5995         89595   \n","5996        5996         25650   \n","5997        5997         28401   \n","5998        5998         83488   \n","5999        5999         82807   \n","\n","                                             lastfm_url  \\\n","0     https://www.last.fm/music/skywave/_/another%2b...   \n","1     https://www.last.fm/music/neneh%2bcherry/_/pea...   \n","2       https://www.last.fm/music/adam%2bgreen/_/goblin   \n","3     https://www.last.fm/music/ceremony/_/i%2bwant%...   \n","4     https://www.last.fm/music/beyond%2bcreation/_/...   \n","...                                                 ...   \n","5995  https://www.last.fm/music/diane%2bbirch/_/noth...   \n","5996  https://www.last.fm/music/tran%2bqual/_/montse...   \n","5997  https://www.last.fm/music/raging%2bslab/_/here...   \n","5998  https://www.last.fm/music/jurassic%2b5/_/back%...   \n","5999  https://www.last.fm/music/landing/_/constellat...   \n","\n","                             track           artist  \\\n","0                     Another Love          Skywave   \n","1                    Peace In Mind     Neneh Cherry   \n","2                           Goblin       Adam Green   \n","3     I Want To Put This To An End         Ceremony   \n","4                      Coexistence  Beyond Creation   \n","...                            ...              ...   \n","5995         Nothing But A Miracle      Diane Birch   \n","5996                    Montserrat        Tran Qual   \n","5997                     Here Lies      Raging Slab   \n","5998                    Back 4 You       Jurassic 5   \n","5999                Constellations          Landing   \n","\n","                                     seeds  number_of_emotion_tags  \\\n","0                ['explosive', 'euphoric']                       5   \n","1                           ['reflective']                       4   \n","2                                ['silly']                       1   \n","3                              ['intense']                       2   \n","4                            ['technical']                       2   \n","...                                    ...                     ...   \n","5995                             ['quiet']                       6   \n","5996                           ['relaxed']                       9   \n","5997  ['sarcastic', 'sardonic', 'cynical']                       4   \n","5998                         ['confident']                       2   \n","5999                            ['spacey']                       2   \n","\n","      valence_tags  arousal_tags  dominance_tags  \\\n","0         5.440000      4.582000        4.548000   \n","1         4.841667      2.815000        4.950000   \n","2         6.720000      4.860000        6.500000   \n","3         3.995000      6.135000        4.585000   \n","4         5.693333      5.476275        6.390392   \n","...            ...           ...             ...   \n","5995      6.062000      3.772000        5.639000   \n","5996      5.326474      3.281561        5.053699   \n","5997      2.742500      3.395000        3.935000   \n","5998      7.386102      4.336271        6.921017   \n","5999      2.860000      2.045000        2.555000   \n","\n","                                      mbid              spotify_id  \\\n","0     3eba75ff-50d1-48eb-b39f-2e1e456c0847                     NaN   \n","1     05011436-be05-433d-b7d1-405f14b45838  3qij053VLKWDprTlF6REDl   \n","2     93f50361-0581-4dd6-a294-93ca5bef904f  122tMrH7PcUWMN4AcklyU0   \n","3     3b09dada-b877-4bda-b35a-34a1d28b1461  3HBwKXoQnPdi8Tzmioiwyv   \n","4     e4348ebc-97f2-4bcb-af2e-2998d0fae905  3TfnNa2rOCuGZi4IBWtYVE   \n","...                                    ...                     ...   \n","5995  db1d9721-6dd4-40b5-89a2-42562e94086e  6IZVRuDF1R3vu7M7x6jAkM   \n","5996                                   NaN                     NaN   \n","5997  dc3b6de4-e085-4f36-a848-d42f4b405f36                     NaN   \n","5998  444318b7-ae48-4dde-a45c-ab3460faeb35                     NaN   \n","5999  65873600-9dc9-40af-899d-39cc9def62f8  2OHK0BPaOX8rBLY3iI38Dn   \n","\n","                      genre  \\\n","0                  shoegaze   \n","1                  trip-hop   \n","2                      folk   \n","3                  hardcore   \n","4     technical death metal   \n","...                     ...   \n","5995                   soul   \n","5996                  indie   \n","5997                   rock   \n","5998                hip-hop   \n","5999              post-rock   \n","\n","                                                 lyrics  binary_valence  \n","0     [Intro]\\r\\nIt's Rittz, bitch\\r\\n\\r\\n[Verse 1]\\...               1  \n","1     I've got a little piece of mind\\r\\nFrom the co...               0  \n","2     When she stopped talking to me\\r\\nI knew that ...               1  \n","3     My heart beats in slow songs, pumping moments ...               0  \n","4     When the universal darkness is covering the ea...               1  \n","...                                                 ...             ...  \n","5995  Gettin' tired of livin'\\r\\nLivin' for a moment...               1  \n","5996                                               None               1  \n","5997                                               None               0  \n","5998  [Intro: Soup]\\r\\nCheck this out, real quick\\r\\...               1  \n","5999  [Verse 1]\\r\\nIn the back of my mind\\r\\nShadows...               0  \n","\n","[6000 rows x 15 columns]"],"text/html":["\n","  <div id=\"df-40bef868-6204-4a9b-a375-d9ed0d44b6b7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>lastfm_url</th>\n","      <th>track</th>\n","      <th>artist</th>\n","      <th>seeds</th>\n","      <th>number_of_emotion_tags</th>\n","      <th>valence_tags</th>\n","      <th>arousal_tags</th>\n","      <th>dominance_tags</th>\n","      <th>mbid</th>\n","      <th>spotify_id</th>\n","      <th>genre</th>\n","      <th>lyrics</th>\n","      <th>binary_valence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2949</td>\n","      <td>https://www.last.fm/music/skywave/_/another%2b...</td>\n","      <td>Another Love</td>\n","      <td>Skywave</td>\n","      <td>['explosive', 'euphoric']</td>\n","      <td>5</td>\n","      <td>5.440000</td>\n","      <td>4.582000</td>\n","      <td>4.548000</td>\n","      <td>3eba75ff-50d1-48eb-b39f-2e1e456c0847</td>\n","      <td>NaN</td>\n","      <td>shoegaze</td>\n","      <td>[Intro]\\r\\nIt's Rittz, bitch\\r\\n\\r\\n[Verse 1]\\...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>65958</td>\n","      <td>https://www.last.fm/music/neneh%2bcherry/_/pea...</td>\n","      <td>Peace In Mind</td>\n","      <td>Neneh Cherry</td>\n","      <td>['reflective']</td>\n","      <td>4</td>\n","      <td>4.841667</td>\n","      <td>2.815000</td>\n","      <td>4.950000</td>\n","      <td>05011436-be05-433d-b7d1-405f14b45838</td>\n","      <td>3qij053VLKWDprTlF6REDl</td>\n","      <td>trip-hop</td>\n","      <td>I've got a little piece of mind\\r\\nFrom the co...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>43234</td>\n","      <td>https://www.last.fm/music/adam%2bgreen/_/goblin</td>\n","      <td>Goblin</td>\n","      <td>Adam Green</td>\n","      <td>['silly']</td>\n","      <td>1</td>\n","      <td>6.720000</td>\n","      <td>4.860000</td>\n","      <td>6.500000</td>\n","      <td>93f50361-0581-4dd6-a294-93ca5bef904f</td>\n","      <td>122tMrH7PcUWMN4AcklyU0</td>\n","      <td>folk</td>\n","      <td>When she stopped talking to me\\r\\nI knew that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>5766</td>\n","      <td>https://www.last.fm/music/ceremony/_/i%2bwant%...</td>\n","      <td>I Want To Put This To An End</td>\n","      <td>Ceremony</td>\n","      <td>['intense']</td>\n","      <td>2</td>\n","      <td>3.995000</td>\n","      <td>6.135000</td>\n","      <td>4.585000</td>\n","      <td>3b09dada-b877-4bda-b35a-34a1d28b1461</td>\n","      <td>3HBwKXoQnPdi8Tzmioiwyv</td>\n","      <td>hardcore</td>\n","      <td>My heart beats in slow songs, pumping moments ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>61410</td>\n","      <td>https://www.last.fm/music/beyond%2bcreation/_/...</td>\n","      <td>Coexistence</td>\n","      <td>Beyond Creation</td>\n","      <td>['technical']</td>\n","      <td>2</td>\n","      <td>5.693333</td>\n","      <td>5.476275</td>\n","      <td>6.390392</td>\n","      <td>e4348ebc-97f2-4bcb-af2e-2998d0fae905</td>\n","      <td>3TfnNa2rOCuGZi4IBWtYVE</td>\n","      <td>technical death metal</td>\n","      <td>When the universal darkness is covering the ea...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5995</th>\n","      <td>5995</td>\n","      <td>89595</td>\n","      <td>https://www.last.fm/music/diane%2bbirch/_/noth...</td>\n","      <td>Nothing But A Miracle</td>\n","      <td>Diane Birch</td>\n","      <td>['quiet']</td>\n","      <td>6</td>\n","      <td>6.062000</td>\n","      <td>3.772000</td>\n","      <td>5.639000</td>\n","      <td>db1d9721-6dd4-40b5-89a2-42562e94086e</td>\n","      <td>6IZVRuDF1R3vu7M7x6jAkM</td>\n","      <td>soul</td>\n","      <td>Gettin' tired of livin'\\r\\nLivin' for a moment...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5996</th>\n","      <td>5996</td>\n","      <td>25650</td>\n","      <td>https://www.last.fm/music/tran%2bqual/_/montse...</td>\n","      <td>Montserrat</td>\n","      <td>Tran Qual</td>\n","      <td>['relaxed']</td>\n","      <td>9</td>\n","      <td>5.326474</td>\n","      <td>3.281561</td>\n","      <td>5.053699</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>indie</td>\n","      <td>None</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5997</th>\n","      <td>5997</td>\n","      <td>28401</td>\n","      <td>https://www.last.fm/music/raging%2bslab/_/here...</td>\n","      <td>Here Lies</td>\n","      <td>Raging Slab</td>\n","      <td>['sarcastic', 'sardonic', 'cynical']</td>\n","      <td>4</td>\n","      <td>2.742500</td>\n","      <td>3.395000</td>\n","      <td>3.935000</td>\n","      <td>dc3b6de4-e085-4f36-a848-d42f4b405f36</td>\n","      <td>NaN</td>\n","      <td>rock</td>\n","      <td>None</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5998</th>\n","      <td>5998</td>\n","      <td>83488</td>\n","      <td>https://www.last.fm/music/jurassic%2b5/_/back%...</td>\n","      <td>Back 4 You</td>\n","      <td>Jurassic 5</td>\n","      <td>['confident']</td>\n","      <td>2</td>\n","      <td>7.386102</td>\n","      <td>4.336271</td>\n","      <td>6.921017</td>\n","      <td>444318b7-ae48-4dde-a45c-ab3460faeb35</td>\n","      <td>NaN</td>\n","      <td>hip-hop</td>\n","      <td>[Intro: Soup]\\r\\nCheck this out, real quick\\r\\...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5999</th>\n","      <td>5999</td>\n","      <td>82807</td>\n","      <td>https://www.last.fm/music/landing/_/constellat...</td>\n","      <td>Constellations</td>\n","      <td>Landing</td>\n","      <td>['spacey']</td>\n","      <td>2</td>\n","      <td>2.860000</td>\n","      <td>2.045000</td>\n","      <td>2.555000</td>\n","      <td>65873600-9dc9-40af-899d-39cc9def62f8</td>\n","      <td>2OHK0BPaOX8rBLY3iI38Dn</td>\n","      <td>post-rock</td>\n","      <td>[Verse 1]\\r\\nIn the back of my mind\\r\\nShadows...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6000 rows Ã— 15 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40bef868-6204-4a9b-a375-d9ed0d44b6b7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-40bef868-6204-4a9b-a375-d9ed0d44b6b7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-40bef868-6204-4a9b-a375-d9ed0d44b6b7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["toDrop = df.index[df[\"lyrics\"] == 'None'].tolist()\n","print(toDrop)\n","df.drop(toDrop, axis = 0, inplace = True)\n","print(df.index[df[\"lyrics\"] =='None'].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"aAwni88jW3pP","executionInfo":{"status":"ok","timestamp":1669753043451,"user_tz":300,"elapsed":31,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"45be7f15-da93-4e81-aec5-f4473e1155bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[5, 6, 10, 11, 13, 14, 15, 17, 19, 20, 21, 25, 26, 31, 42, 47, 48, 50, 52, 61, 62, 68, 69, 74, 76, 85, 107, 108, 116, 117, 123, 129, 132, 133, 135, 136, 138, 141, 145, 148, 158, 161, 162, 163, 166, 167, 173, 180, 181, 186, 187, 189, 190, 198, 199, 200, 202, 203, 204, 206, 210, 212, 214, 215, 218, 219, 221, 222, 230, 232, 235, 236, 240, 241, 247, 249, 250, 254, 255, 258, 259, 261, 264, 268, 275, 277, 278, 279, 281, 285, 287, 290, 291, 293, 295, 297, 306, 309, 314, 330, 333, 334, 335, 336, 337, 343, 345, 347, 348, 351, 353, 355, 357, 358, 359, 361, 365, 366, 372, 374, 376, 377, 382, 385, 386, 388, 395, 402, 404, 405, 406, 407, 408, 410, 412, 413, 418, 422, 425, 428, 430, 440, 449, 451, 452, 456, 457, 459, 460, 462, 464, 466, 468, 475, 476, 477, 480, 483, 486, 487, 491, 495, 506, 516, 518, 520, 524, 526, 527, 530, 537, 539, 544, 546, 548, 549, 553, 555, 556, 560, 561, 562, 564, 566, 569, 570, 571, 572, 574, 575, 576, 579, 583, 584, 587, 591, 596, 598, 606, 609, 610, 611, 613, 614, 617, 620, 625, 631, 633, 635, 641, 648, 650, 653, 654, 658, 661, 674, 677, 678, 679, 681, 683, 684, 686, 690, 691, 692, 703, 705, 711, 717, 720, 724, 729, 732, 734, 737, 738, 742, 743, 753, 754, 756, 759, 761, 763, 765, 767, 770, 771, 775, 776, 779, 787, 790, 791, 795, 801, 803, 804, 809, 810, 813, 814, 815, 818, 823, 824, 830, 831, 832, 833, 835, 836, 840, 849, 850, 853, 854, 856, 860, 865, 874, 878, 880, 882, 894, 895, 897, 899, 900, 907, 908, 909, 913, 924, 925, 928, 929, 931, 932, 940, 942, 944, 955, 957, 959, 963, 966, 969, 970, 978, 986, 987, 988, 989, 996, 1000, 1001, 1002, 1005, 1009, 1011, 1015, 1023, 1032, 1033, 1035, 1038, 1040, 1041, 1042, 1043, 1049, 1052, 1056, 1061, 1066, 1067, 1070, 1075, 1076, 1090, 1094, 1100, 1103, 1108, 1110, 1116, 1118, 1124, 1127, 1138, 1139, 1142, 1144, 1147, 1155, 1158, 1163, 1170, 1171, 1172, 1176, 1177, 1186, 1194, 1195, 1196, 1197, 1198, 1202, 1205, 1206, 1215, 1216, 1217, 1218, 1220, 1223, 1225, 1227, 1228, 1234, 1240, 1242, 1243, 1245, 1252, 1255, 1257, 1258, 1260, 1266, 1269, 1270, 1271, 1272, 1275, 1276, 1289, 1290, 1293, 1296, 1297, 1300, 1306, 1308, 1309, 1310, 1313, 1315, 1316, 1323, 1325, 1330, 1332, 1335, 1339, 1348, 1351, 1353, 1357, 1359, 1362, 1364, 1366, 1368, 1369, 1371, 1373, 1374, 1375, 1382, 1383, 1385, 1386, 1393, 1399, 1400, 1409, 1410, 1412, 1413, 1414, 1415, 1417, 1422, 1433, 1435, 1436, 1438, 1446, 1447, 1450, 1456, 1458, 1462, 1467, 1469, 1470, 1472, 1479, 1482, 1484, 1491, 1495, 1499, 1506, 1507, 1513, 1514, 1519, 1521, 1522, 1524, 1526, 1528, 1529, 1531, 1533, 1536, 1539, 1541, 1542, 1546, 1547, 1553, 1554, 1555, 1562, 1564, 1565, 1566, 1575, 1576, 1578, 1583, 1588, 1593, 1596, 1597, 1599, 1600, 1601, 1602, 1610, 1612, 1613, 1616, 1621, 1622, 1628, 1632, 1637, 1638, 1642, 1643, 1644, 1645, 1646, 1657, 1658, 1661, 1663, 1668, 1672, 1673, 1674, 1677, 1684, 1685, 1688, 1689, 1690, 1691, 1693, 1700, 1704, 1705, 1706, 1708, 1710, 1718, 1729, 1732, 1740, 1745, 1752, 1754, 1755, 1765, 1766, 1767, 1770, 1773, 1775, 1776, 1779, 1782, 1795, 1796, 1798, 1799, 1801, 1812, 1814, 1816, 1818, 1819, 1821, 1823, 1824, 1826, 1827, 1832, 1835, 1847, 1851, 1853, 1854, 1855, 1856, 1860, 1865, 1868, 1869, 1870, 1875, 1876, 1885, 1888, 1897, 1899, 1910, 1911, 1915, 1917, 1928, 1937, 1939, 1944, 1947, 1951, 1952, 1963, 1970, 1972, 1974, 1981, 1987, 1998, 2001, 2003, 2006, 2008, 2012, 2013, 2017, 2018, 2021, 2025, 2029, 2031, 2038, 2041, 2042, 2048, 2049, 2052, 2057, 2059, 2061, 2062, 2063, 2065, 2067, 2072, 2077, 2078, 2080, 2084, 2087, 2088, 2093, 2102, 2106, 2110, 2113, 2114, 2115, 2117, 2120, 2127, 2130, 2132, 2133, 2137, 2139, 2142, 2145, 2149, 2150, 2154, 2156, 2163, 2164, 2166, 2168, 2173, 2175, 2178, 2179, 2181, 2182, 2185, 2188, 2191, 2202, 2211, 2212, 2216, 2217, 2225, 2228, 2231, 2232, 2234, 2236, 2237, 2241, 2242, 2244, 2245, 2254, 2258, 2259, 2267, 2268, 2270, 2273, 2277, 2287, 2292, 2293, 2294, 2299, 2301, 2303, 2304, 2305, 2311, 2316, 2319, 2321, 2322, 2324, 2326, 2327, 2337, 2347, 2349, 2351, 2353, 2354, 2357, 2361, 2363, 2365, 2367, 2369, 2371, 2373, 2376, 2378, 2379, 2381, 2382, 2391, 2392, 2394, 2395, 2396, 2399, 2400, 2404, 2405, 2406, 2410, 2411, 2413, 2414, 2417, 2418, 2426, 2429, 2433, 2440, 2441, 2448, 2451, 2452, 2457, 2458, 2459, 2460, 2461, 2462, 2468, 2470, 2474, 2476, 2483, 2485, 2487, 2490, 2494, 2495, 2496, 2499, 2502, 2503, 2504, 2508, 2510, 2515, 2518, 2519, 2520, 2529, 2530, 2531, 2532, 2534, 2537, 2538, 2540, 2542, 2547, 2548, 2554, 2562, 2563, 2566, 2568, 2573, 2575, 2576, 2577, 2579, 2581, 2583, 2589, 2593, 2594, 2595, 2597, 2602, 2604, 2614, 2617, 2620, 2624, 2633, 2637, 2639, 2642, 2643, 2646, 2650, 2651, 2653, 2654, 2658, 2659, 2663, 2664, 2665, 2666, 2668, 2670, 2671, 2674, 2675, 2677, 2680, 2687, 2691, 2694, 2695, 2697, 2699, 2707, 2708, 2710, 2711, 2712, 2713, 2718, 2725, 2729, 2731, 2743, 2750, 2756, 2758, 2759, 2763, 2764, 2765, 2775, 2780, 2783, 2804, 2808, 2811, 2814, 2816, 2817, 2821, 2823, 2827, 2831, 2834, 2838, 2841, 2842, 2845, 2847, 2848, 2853, 2854, 2856, 2857, 2860, 2861, 2864, 2867, 2868, 2869, 2873, 2875, 2880, 2881, 2883, 2887, 2888, 2889, 2891, 2892, 2895, 2896, 2897, 2898, 2900, 2903, 2905, 2906, 2909, 2912, 2913, 2914, 2917, 2920, 2924, 2928, 2930, 2932, 2936, 2937, 2940, 2942, 2952, 2955, 2957, 2958, 2959, 2964, 2967, 2969, 2970, 2975, 2976, 2979, 2983, 2984, 2985, 2986, 2987, 2989, 2990, 2996, 2997, 3001, 3003, 3004, 3009, 3013, 3016, 3020, 3023, 3024, 3026, 3027, 3029, 3033, 3034, 3036, 3037, 3038, 3039, 3042, 3047, 3048, 3050, 3054, 3061, 3063, 3066, 3068, 3070, 3074, 3076, 3078, 3079, 3086, 3088, 3089, 3092, 3094, 3098, 3101, 3105, 3107, 3110, 3112, 3117, 3118, 3120, 3124, 3127, 3128, 3129, 3133, 3140, 3142, 3143, 3147, 3151, 3159, 3160, 3161, 3163, 3164, 3166, 3167, 3169, 3171, 3172, 3179, 3180, 3181, 3185, 3187, 3191, 3193, 3194, 3195, 3196, 3204, 3205, 3206, 3210, 3211, 3212, 3217, 3222, 3225, 3233, 3237, 3239, 3241, 3242, 3246, 3247, 3251, 3252, 3253, 3254, 3265, 3267, 3269, 3273, 3275, 3276, 3277, 3279, 3283, 3284, 3285, 3289, 3294, 3299, 3303, 3305, 3307, 3309, 3314, 3331, 3334, 3340, 3344, 3346, 3360, 3364, 3365, 3367, 3368, 3370, 3374, 3380, 3383, 3388, 3389, 3394, 3395, 3396, 3401, 3403, 3405, 3407, 3409, 3414, 3415, 3417, 3418, 3423, 3424, 3425, 3428, 3431, 3433, 3435, 3442, 3443, 3448, 3451, 3452, 3456, 3457, 3458, 3459, 3461, 3463, 3469, 3478, 3483, 3490, 3491, 3492, 3495, 3498, 3500, 3502, 3506, 3511, 3512, 3517, 3519, 3523, 3529, 3531, 3532, 3537, 3543, 3544, 3546, 3547, 3548, 3549, 3551, 3553, 3555, 3562, 3571, 3578, 3579, 3581, 3584, 3587, 3588, 3589, 3591, 3592, 3596, 3598, 3599, 3600, 3606, 3607, 3614, 3620, 3622, 3625, 3627, 3631, 3632, 3635, 3639, 3641, 3642, 3646, 3651, 3655, 3656, 3658, 3659, 3660, 3667, 3668, 3670, 3672, 3673, 3678, 3683, 3684, 3686, 3691, 3695, 3698, 3702, 3709, 3714, 3717, 3718, 3720, 3721, 3722, 3723, 3725, 3744, 3745, 3753, 3756, 3758, 3762, 3765, 3767, 3773, 3774, 3776, 3779, 3783, 3785, 3787, 3792, 3793, 3794, 3805, 3806, 3813, 3815, 3820, 3827, 3828, 3831, 3834, 3836, 3838, 3842, 3850, 3854, 3856, 3859, 3861, 3862, 3871, 3872, 3873, 3876, 3881, 3885, 3886, 3887, 3889, 3891, 3897, 3898, 3899, 3906, 3909, 3917, 3918, 3921, 3922, 3924, 3925, 3927, 3930, 3931, 3933, 3936, 3937, 3938, 3940, 3942, 3946, 3950, 3960, 3962, 3963, 3965, 3966, 3970, 3972, 3975, 3976, 3978, 3979, 3982, 3983, 3987, 3992, 3995, 3996, 3997, 3999, 4001, 4004, 4011, 4016, 4019, 4022, 4024, 4026, 4033, 4035, 4037, 4040, 4047, 4054, 4055, 4056, 4058, 4061, 4064, 4071, 4082, 4087, 4088, 4089, 4101, 4106, 4115, 4116, 4120, 4125, 4127, 4129, 4132, 4133, 4137, 4140, 4146, 4147, 4148, 4149, 4151, 4152, 4153, 4154, 4159, 4162, 4163, 4166, 4167, 4169, 4171, 4172, 4175, 4176, 4178, 4181, 4185, 4186, 4187, 4191, 4196, 4199, 4202, 4207, 4208, 4216, 4223, 4224, 4225, 4228, 4229, 4230, 4236, 4237, 4238, 4239, 4240, 4242, 4245, 4254, 4255, 4262, 4265, 4270, 4271, 4275, 4276, 4277, 4281, 4286, 4291, 4297, 4308, 4313, 4318, 4320, 4323, 4327, 4329, 4330, 4333, 4343, 4345, 4348, 4349, 4350, 4352, 4355, 4358, 4365, 4366, 4374, 4381, 4386, 4387, 4391, 4392, 4393, 4398, 4403, 4406, 4414, 4416, 4420, 4422, 4427, 4430, 4431, 4435, 4438, 4439, 4441, 4442, 4446, 4451, 4453, 4457, 4460, 4461, 4462, 4468, 4474, 4479, 4480, 4481, 4486, 4494, 4497, 4499, 4502, 4508, 4509, 4526, 4530, 4532, 4533, 4536, 4543, 4545, 4547, 4549, 4551, 4552, 4555, 4557, 4558, 4560, 4562, 4565, 4566, 4567, 4569, 4570, 4572, 4576, 4579, 4584, 4590, 4593, 4596, 4597, 4599, 4603, 4605, 4610, 4618, 4619, 4621, 4622, 4623, 4632, 4633, 4636, 4642, 4647, 4650, 4651, 4652, 4653, 4656, 4657, 4658, 4661, 4662, 4666, 4668, 4674, 4675, 4677, 4690, 4697, 4700, 4701, 4702, 4704, 4705, 4706, 4709, 4711, 4712, 4714, 4723, 4724, 4729, 4738, 4740, 4741, 4743, 4751, 4755, 4756, 4762, 4769, 4770, 4778, 4780, 4782, 4799, 4809, 4812, 4816, 4817, 4821, 4833, 4835, 4838, 4839, 4844, 4846, 4850, 4852, 4853, 4854, 4858, 4867, 4871, 4878, 4880, 4887, 4889, 4892, 4893, 4895, 4903, 4906, 4908, 4910, 4911, 4917, 4921, 4922, 4924, 4925, 4934, 4937, 4938, 4940, 4944, 4945, 4948, 4953, 4956, 4958, 4965, 4966, 4967, 4969, 4973, 4974, 4977, 4980, 4981, 4983, 4987, 4988, 4990, 4995, 5003, 5004, 5006, 5009, 5010, 5012, 5013, 5016, 5018, 5023, 5026, 5038, 5040, 5044, 5046, 5049, 5051, 5054, 5055, 5056, 5063, 5071, 5072, 5073, 5074, 5076, 5077, 5081, 5082, 5085, 5087, 5089, 5091, 5095, 5096, 5098, 5102, 5112, 5116, 5119, 5120, 5121, 5122, 5123, 5130, 5133, 5136, 5138, 5140, 5142, 5149, 5151, 5152, 5154, 5156, 5157, 5162, 5165, 5171, 5172, 5173, 5178, 5184, 5196, 5201, 5211, 5231, 5239, 5242, 5243, 5246, 5255, 5258, 5262, 5264, 5265, 5268, 5271, 5273, 5275, 5276, 5281, 5283, 5284, 5291, 5296, 5297, 5298, 5302, 5305, 5309, 5310, 5311, 5313, 5315, 5316, 5317, 5319, 5320, 5322, 5326, 5327, 5328, 5330, 5335, 5341, 5345, 5346, 5353, 5354, 5362, 5365, 5367, 5369, 5375, 5376, 5380, 5382, 5388, 5389, 5390, 5391, 5392, 5394, 5396, 5398, 5399, 5401, 5410, 5412, 5415, 5417, 5418, 5419, 5422, 5429, 5437, 5438, 5440, 5444, 5460, 5466, 5467, 5470, 5479, 5483, 5487, 5488, 5489, 5496, 5497, 5502, 5509, 5510, 5514, 5515, 5528, 5531, 5533, 5535, 5536, 5538, 5545, 5546, 5547, 5549, 5550, 5562, 5563, 5569, 5576, 5579, 5588, 5589, 5595, 5597, 5604, 5605, 5608, 5609, 5611, 5613, 5618, 5626, 5629, 5633, 5634, 5636, 5639, 5641, 5642, 5651, 5653, 5654, 5657, 5661, 5662, 5665, 5671, 5676, 5678, 5679, 5685, 5693, 5695, 5696, 5699, 5709, 5711, 5715, 5717, 5719, 5722, 5723, 5726, 5728, 5729, 5730, 5734, 5737, 5738, 5747, 5748, 5749, 5752, 5760, 5768, 5771, 5772, 5782, 5786, 5788, 5789, 5790, 5794, 5796, 5798, 5799, 5801, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5816, 5817, 5818, 5822, 5826, 5831, 5839, 5842, 5843, 5849, 5851, 5853, 5858, 5865, 5871, 5876, 5878, 5879, 5884, 5888, 5890, 5892, 5898, 5902, 5909, 5910, 5911, 5913, 5915, 5922, 5927, 5929, 5931, 5944, 5945, 5946, 5950, 5952, 5955, 5958, 5959, 5963, 5966, 5967, 5974, 5975, 5977, 5980, 5981, 5988, 5992, 5993, 5994, 5996, 5997]\n","[]\n"]}]},{"cell_type":"code","source":["y = df['binary_valence'].to_numpy()\n","X = df['lyrics'].to_numpy()\n"],"metadata":{"id":"AV3UwOkLXEdy","executionInfo":{"status":"ok","timestamp":1669759220026,"user_tz":300,"elapsed":121,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"DsbSUKR0b62P","executionInfo":{"status":"ok","timestamp":1669753050283,"user_tz":300,"elapsed":568,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"d508a492-4309-4efe-e151-73c4d21c79d9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## Tokenizing and Embeedding\n","\n","the data was tokenized using nltk and then, using googles pretrained word2vec model, was converted into word embeddings to better be able to classify the data. "],"metadata":{"id":"xGzxx5bdZ-Rw"}},{"cell_type":"code","source":["delete_list = ['Intro','Verse','Hook']\n","X_tokenized = []\n","for lyrics in X:\n","  sent_tok = nltk.sent_tokenize(lyrics)\n","  word_tok = [nltk.word_tokenize(i) for i in sent_tok]\n","  word_tok = [word.lower() for sublist in word_tok for word in sublist if (word.isalpha() and word not in delete_list)]\n","  X_tokenized.append(word_tok)"],"metadata":{"id":"3SUJPb7QYx6R","executionInfo":{"status":"ok","timestamp":1669753057075,"user_tz":300,"elapsed":6129,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["bigmodel = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLPproject/GoogleNews-vectors-negative300-SLIM.bin.gz', binary=True)\n"],"metadata":{"id":"ePiMsm0Ehfb1","executionInfo":{"status":"ok","timestamp":1669753068837,"user_tz":300,"elapsed":11776,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["song_vecs = []\n","\n","for toks in X_tokenized:\n","  totvec = np.zeros(300)\n","  for tok in toks:\n","    if tok.lower() in bigmodel:\n","      totvec += bigmodel[tok.lower()]\n","  song_vecs.append(totvec)\n","\n","print(len(song_vecs))\n","print(len(song_vecs[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Rqh1TtThi3i0","executionInfo":{"status":"ok","timestamp":1669753073140,"user_tz":300,"elapsed":4320,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"963aa53f-2cd3-4720-b012-40e1ce000e45"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["4111\n","300\n"]}]},{"cell_type":"markdown","source":["## Getting Word Embedding Data Ready for Model\n","\n","In order to have an efficient way of training and testing, the data was first shuffled and then divided in an 80-20 split (training-testing). Afterwards, the data was then supplied into a DataLoader object for proper interaction with the pytorch model later on."],"metadata":{"id":"PgUPfB5HXIuq"}},{"cell_type":"code","source":["indexes = np.array(range(len(y)))\n","np.random.shuffle(indexes)\n","indexes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Irs6dxVqq-PV","executionInfo":{"status":"ok","timestamp":1669753073149,"user_tz":300,"elapsed":121,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"70642602-2c7d-4b2a-d759-5b295a851b9e"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1960, 2940,   69, ..., 2261, 4067,  722])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["y_shuffled = []\n","X_shuffled = []\n","dataset = []\n","shuffled_dataset = []\n","for i in range(len(y)):\n","  dataset.append((torch.tensor(song_vecs[i]),y[i]))\n","for i in range(len(dataset)):\n","  shuffled_dataset.append(dataset[indexes[i]])\n","\n","train_set = dataset[0:int(len(shuffled_dataset) * .8)]\n","test_set = dataset[0:int(len(shuffled_dataset) * .2)]"],"metadata":{"id":"cdH4W4bHrt5C","executionInfo":{"status":"ok","timestamp":1669753073152,"user_tz":300,"elapsed":117,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainset = torch.utils.data.DataLoader(train_set,batch_size = 5, shuffle = True, num_workers = 2)\n","testset = torch.utils.data.DataLoader(test_set,batch_size = 5, shuffle = False, num_workers = 2)\n","classes = ('sad','happy')"],"metadata":{"id":"7yNbYeZ8x1Zr","executionInfo":{"status":"ok","timestamp":1669753156796,"user_tz":300,"elapsed":254,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## Feed Forward Neural Network Training and Fitting\n","\n","For training this dataset a feed forward neural network was created. This network contains 5 layers, each being normalized by the relu function. The loss function used for this is cross entropy. Training, testing and recording of validation is constructed all in the class architecture.\n","\n","For parameter initialization, the length of one input is used for input while the output is set to the class size, 2. Afterwards, the model is then sent to the device and  then further trained and tested. \n","\n","An average accuracy of 92% was achieved with a loss of 23%."],"metadata":{"id":"5dL6UILXXPoA"}},{"cell_type":"code","source":["class Forward_NN(nn.Module):\n","  def __init__(self,input,output):\n","    super().__init__()\n","    self.layer1 = nn.Linear(input, input)\n","    self.layer2 = nn.Linear(input,input)\n","    self.layer3 = nn.Linear(input,input)\n","    self.layer4 = nn.Linear(input,100)\n","    self.layer5 = nn.Linear(100,output)\n","  \n","  def forward(self, input):\n","    out = self.layer1(input)\n","    out = F.relu(out)\n","    out = self.layer2(out)\n","    out = F.relu(out)\n","    out = self.layer3(out)\n","    out = F.relu(out) \n","    out = self.layer4(out)\n","    out = F.relu(out)\n","    output = self.layer5(out)\n","    return output\n","  \n","  def training_step(self,batch):\n","    input,labels = batch\n","    input = input.to(device)\n","    labels = labels.to(device)\n","    out = self(input.float())\n","    loss = F.cross_entropy(out,labels)\n","    return loss\n","  \n","  def validation_step(self,batch):\n","    input, labels = batch\n","    input = input.to(device)\n","    labels = labels.to(device)\n","    out = self(input.float())\n","    loss = F.cross_entropy(out,labels)\n","    _, preds = torch.max(out, dim = 1)\n","    acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","    return {'loss':loss,'acc':acc}\n","\n","  def epoch_val_end(self,outputs):\n","    batch_losses = [x['loss'] for x in outputs]\n","    epoch_loss = torch.stack(batch_losses).mean()   \n","    batch_accs = [x['acc'] for x in outputs]\n","    epoch_acc = torch.stack(batch_accs).mean()      \n","    return {'loss': epoch_loss.item(), 'acc': epoch_acc.item()}\n","\n","  def epoch_end(self, epoch, result):\n","    print(\"Epoch [{}], loss: {:.4f}, acc: {:.4f}\".format(epoch, result['loss'], result['acc']))\n","  \n","\n"],"metadata":{"id":"hMD23Ph8lumc","executionInfo":{"status":"ok","timestamp":1669753244247,"user_tz":300,"elapsed":240,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["fnn = Forward_NN(len(song_vecs[0]), 2)\n","device = torch.device('cuda')\n","fnn = fnn.to(device)"],"metadata":{"id":"KYESJGwJqmfa","executionInfo":{"status":"ok","timestamp":1669758377410,"user_tz":300,"elapsed":142,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["history = []\n","optimizer = torch.optim.SGD(fnn.parameters(),0.001)\n","for epoch in range(100):\n","  for batch in trainset:\n","    loss = fnn.training_step(batch)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","  outputs = [fnn.validation_step(batch) for batch in testset]\n","  results = fnn.epoch_val_end(outputs)\n","  fnn.epoch_end(epoch,results)\n","  history.append(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"jYjLGlkb3O-4","executionInfo":{"status":"ok","timestamp":1669753877619,"user_tz":300,"elapsed":257107,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"718b8acc-77be-48b3-8948-2ca2ec587ebd"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0], loss: 0.4047, acc: 0.8170\n","Epoch [1], loss: 0.4764, acc: 0.7400\n","Epoch [2], loss: 0.4603, acc: 0.7418\n","Epoch [3], loss: 0.4043, acc: 0.8073\n","Epoch [4], loss: 0.4538, acc: 0.7618\n","Epoch [5], loss: 0.4256, acc: 0.7879\n","Epoch [6], loss: 0.4323, acc: 0.7939\n","Epoch [7], loss: 0.3906, acc: 0.8291\n","Epoch [8], loss: 0.4637, acc: 0.7697\n","Epoch [9], loss: 0.3817, acc: 0.8091\n","Epoch [10], loss: 0.3792, acc: 0.8448\n","Epoch [11], loss: 0.3976, acc: 0.8170\n","Epoch [12], loss: 0.3427, acc: 0.8545\n","Epoch [13], loss: 0.3855, acc: 0.8364\n","Epoch [14], loss: 0.3424, acc: 0.8679\n","Epoch [15], loss: 0.5635, acc: 0.7206\n","Epoch [16], loss: 0.3691, acc: 0.8509\n","Epoch [17], loss: 0.3507, acc: 0.8491\n","Epoch [18], loss: 0.4863, acc: 0.7673\n","Epoch [19], loss: 0.3467, acc: 0.8582\n","Epoch [20], loss: 0.3668, acc: 0.8279\n","Epoch [21], loss: 0.5061, acc: 0.7661\n","Epoch [22], loss: 0.3891, acc: 0.8267\n","Epoch [23], loss: 0.5951, acc: 0.6642\n","Epoch [24], loss: 0.3045, acc: 0.8655\n","Epoch [25], loss: 0.3216, acc: 0.8655\n","Epoch [26], loss: 0.4355, acc: 0.7958\n","Epoch [27], loss: 0.2859, acc: 0.9103\n","Epoch [28], loss: 0.2700, acc: 0.8921\n","Epoch [29], loss: 0.2764, acc: 0.8994\n","Epoch [30], loss: 0.3697, acc: 0.8485\n","Epoch [31], loss: 0.2999, acc: 0.8642\n","Epoch [32], loss: 0.2556, acc: 0.8909\n","Epoch [33], loss: 0.2717, acc: 0.8982\n","Epoch [34], loss: 0.4099, acc: 0.8048\n","Epoch [35], loss: 0.2934, acc: 0.8764\n","Epoch [36], loss: 0.3376, acc: 0.8558\n","Epoch [37], loss: 0.2897, acc: 0.8861\n","Epoch [38], loss: 0.2346, acc: 0.9006\n","Epoch [39], loss: 0.2284, acc: 0.9382\n","Epoch [40], loss: 0.3158, acc: 0.8642\n","Epoch [41], loss: 0.2586, acc: 0.8885\n","Epoch [42], loss: 0.2149, acc: 0.9152\n","Epoch [43], loss: 0.1888, acc: 0.9418\n","Epoch [44], loss: 0.8132, acc: 0.7618\n","Epoch [45], loss: 0.1999, acc: 0.9248\n","Epoch [46], loss: 0.2434, acc: 0.8970\n","Epoch [47], loss: 0.2529, acc: 0.8945\n","Epoch [48], loss: 0.1749, acc: 0.9358\n","Epoch [49], loss: 0.1988, acc: 0.9345\n","Epoch [50], loss: 0.1811, acc: 0.9333\n","Epoch [51], loss: 0.2843, acc: 0.8764\n","Epoch [52], loss: 0.2696, acc: 0.8861\n","Epoch [53], loss: 0.4047, acc: 0.8097\n","Epoch [54], loss: 0.2292, acc: 0.9164\n","Epoch [55], loss: 0.1973, acc: 0.9152\n","Epoch [56], loss: 0.2508, acc: 0.9042\n","Epoch [57], loss: 0.1917, acc: 0.9248\n","Epoch [58], loss: 0.1567, acc: 0.9467\n","Epoch [59], loss: 0.1440, acc: 0.9576\n","Epoch [60], loss: 0.1286, acc: 0.9709\n","Epoch [61], loss: 0.1855, acc: 0.9345\n","Epoch [62], loss: 0.2682, acc: 0.8873\n","Epoch [63], loss: 0.1443, acc: 0.9564\n","Epoch [64], loss: 0.3357, acc: 0.8679\n","Epoch [65], loss: 0.4482, acc: 0.7982\n","Epoch [66], loss: 0.2080, acc: 0.9273\n","Epoch [67], loss: 0.2303, acc: 0.9018\n","Epoch [68], loss: 0.1882, acc: 0.9370\n","Epoch [69], loss: 1.0804, acc: 0.5424\n","Epoch [70], loss: 0.2468, acc: 0.8885\n","Epoch [71], loss: 0.1720, acc: 0.9382\n","Epoch [72], loss: 0.1855, acc: 0.9224\n","Epoch [73], loss: 0.2117, acc: 0.9188\n","Epoch [74], loss: 0.1487, acc: 0.9515\n","Epoch [75], loss: 0.1542, acc: 0.9539\n","Epoch [76], loss: 0.1415, acc: 0.9503\n","Epoch [77], loss: 0.1380, acc: 0.9539\n","Epoch [78], loss: 0.1577, acc: 0.9442\n","Epoch [79], loss: 0.1478, acc: 0.9491\n","Epoch [80], loss: 0.1217, acc: 0.9624\n","Epoch [81], loss: 0.1313, acc: 0.9600\n","Epoch [82], loss: 0.0889, acc: 0.9721\n","Epoch [83], loss: 0.1327, acc: 0.9612\n","Epoch [84], loss: 0.1609, acc: 0.9382\n","Epoch [85], loss: 0.1290, acc: 0.9564\n","Epoch [86], loss: 0.0604, acc: 0.9867\n","Epoch [87], loss: 1.7228, acc: 0.7594\n","Epoch [88], loss: 0.1341, acc: 0.9515\n","Epoch [89], loss: 0.0794, acc: 0.9794\n","Epoch [90], loss: 0.1822, acc: 0.9236\n","Epoch [91], loss: 0.1993, acc: 0.9176\n","Epoch [92], loss: 0.0520, acc: 0.9867\n","Epoch [93], loss: 0.0549, acc: 0.9891\n","Epoch [94], loss: 0.1654, acc: 0.9309\n","Epoch [95], loss: 0.0884, acc: 0.9709\n","Epoch [96], loss: 0.2095, acc: 0.9127\n","Epoch [97], loss: 0.0382, acc: 0.9939\n","Epoch [98], loss: 0.7035, acc: 0.8436\n","Epoch [99], loss: 0.2277, acc: 0.9042\n"]}]},{"cell_type":"code","source":["feed_forward_df = pd.DataFrame(data = history)\n","accuracy = np.mean(feed_forward_df.acc[50:].to_numpy())\n","loss = np.mean(feed_forward_df.loss[50:].to_numpy())\n","print('For the last 50 inputs, accuracy was {:.4f} and loss was {:.4f}'.format(accuracy,loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6OGpFHQVF0E0","executionInfo":{"status":"ok","timestamp":1669754253068,"user_tz":300,"elapsed":163,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"c55fc7fa-9959-4870-ac7f-5e827b10d238"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["For the last 50 inputs, accuracy was 0.9193 and loss was 0.2383\n"]}]},{"cell_type":"markdown","source":["## Feed Forward Neural Network with Normalization Training and Fitting\n","\n","In order to avoid overfitting the model onto the training data, the previously used feed forward architecture was changed. In this version, 4 layers are used and batch normalization and dropout are added to aid against normalized.\n","\n","After this distinction is made, the same format is followed as above for training and sending the device to the GPU.\n","\n","An accuracy of 68% was achieved with a loss of 60%"],"metadata":{"id":"KrEhv0vjXWm2"}},{"cell_type":"code","source":["class fnn_normalized(nn.Module):\n","  def __init__(self,input,output):\n","    super().__init__()\n","    self.layer1 = nn.Linear(input, 64)\n","    self.layer2 = nn.Linear(64,64)\n","    self.layer3 = nn.Linear(64,50)\n","    self.layer4 = nn.Linear(50,output)\n","    self.dropout = nn.Dropout(p = 0.1)\n","    self.bn1 = nn.BatchNorm1d(64)\n","    self.bn2 = nn.BatchNorm1d(64)\n","    self.bn3 = nn.BatchNorm1d(50)\n","  \n","  def forward(self, input):\n","    out = self.layer1(input)\n","    out = F.relu(out)\n","    out = self.bn1(out)\n","    out = self.layer2(out)\n","    out = F.relu(out)\n","    out = self.bn2(out)\n","    out = self.layer3(out)\n","    out = F.relu(out)\n","    out = self.bn3(out)\n","    out = self.dropout(out)\n","    out = self.layer4(out)\n","    return out\n","  \n","  def training_step(self,batch):\n","    input,labels = batch\n","    input = input.to(device)\n","    labels = labels.to(device)\n","    out = self(input.float())\n","    loss = F.cross_entropy(out,labels)\n","    return loss\n","  \n","  def validation_step(self,batch):\n","    input, labels = batch\n","    input = input.to(device)\n","    labels = labels.to(device)\n","    out = self(input.float())\n","    loss = F.cross_entropy(out,labels)\n","    _, preds = torch.max(out, dim = 1)\n","    acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","    return {'loss':loss,'acc':acc}\n","\n","  def epoch_val_end(self,outputs):\n","    batch_losses = [x['loss'] for x in outputs]\n","    epoch_loss = torch.stack(batch_losses).mean()   \n","    batch_accs = [x['acc'] for x in outputs]\n","    epoch_acc = torch.stack(batch_accs).mean()      \n","    return {'loss': epoch_loss.item(), 'acc': epoch_acc.item()}\n","\n","  def epoch_end(self, epoch, result):\n","    print(\"Epoch [{}], loss: {:.4f}, acc: {:.4f}\".format(epoch, result['loss'], result['acc']))\n","  \n"],"metadata":{"id":"pZEYzAH3KXMy","executionInfo":{"status":"ok","timestamp":1669757543110,"user_tz":300,"elapsed":128,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["fnn_n = fnn_normalized(len(song_vecs[0]), 2)\n","fnn_n = fnn_n.to(device)"],"metadata":{"id":"P_64djE0QnP-","executionInfo":{"status":"ok","timestamp":1669757549020,"user_tz":300,"elapsed":130,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["history = []\n","optimizer = torch.optim.SGD(fnn_n.parameters(),0.001)\n","for epoch in range(100):\n","  for batch in trainset:\n","    loss = fnn_n.training_step(batch)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","  outputs = [fnn_n.validation_step(batch) for batch in testset]\n","  results = fnn_n.epoch_val_end(outputs)\n","  fnn_n.epoch_end(epoch,results)\n","  history.append(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRVddwfIROdK","executionInfo":{"status":"ok","timestamp":1669758232087,"user_tz":300,"elapsed":308951,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"fc3f6ad6-1e06-4012-ed52-01846400b645"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0], loss: 0.6047, acc: 0.6745\n","Epoch [1], loss: 0.6038, acc: 0.6867\n","Epoch [2], loss: 0.6022, acc: 0.6782\n","Epoch [3], loss: 0.6106, acc: 0.6818\n","Epoch [4], loss: 0.6197, acc: 0.6745\n","Epoch [5], loss: 0.6040, acc: 0.6879\n","Epoch [6], loss: 0.6160, acc: 0.6685\n","Epoch [7], loss: 0.6147, acc: 0.6685\n","Epoch [8], loss: 0.6179, acc: 0.6770\n","Epoch [9], loss: 0.6148, acc: 0.6915\n","Epoch [10], loss: 0.6081, acc: 0.6891\n","Epoch [11], loss: 0.6124, acc: 0.6891\n","Epoch [12], loss: 0.6127, acc: 0.6830\n","Epoch [13], loss: 0.6115, acc: 0.6721\n","Epoch [14], loss: 0.6045, acc: 0.6915\n","Epoch [15], loss: 0.6092, acc: 0.6855\n","Epoch [16], loss: 0.6040, acc: 0.6879\n","Epoch [17], loss: 0.6040, acc: 0.6927\n","Epoch [18], loss: 0.6035, acc: 0.6855\n","Epoch [19], loss: 0.6209, acc: 0.6673\n","Epoch [20], loss: 0.6172, acc: 0.6745\n","Epoch [21], loss: 0.6049, acc: 0.6939\n","Epoch [22], loss: 0.6087, acc: 0.6891\n","Epoch [23], loss: 0.6105, acc: 0.6758\n","Epoch [24], loss: 0.6105, acc: 0.6879\n","Epoch [25], loss: 0.6083, acc: 0.6952\n","Epoch [26], loss: 0.6127, acc: 0.6818\n","Epoch [27], loss: 0.6079, acc: 0.6758\n","Epoch [28], loss: 0.6136, acc: 0.6758\n","Epoch [29], loss: 0.6105, acc: 0.6794\n","Epoch [30], loss: 0.6107, acc: 0.6770\n","Epoch [31], loss: 0.6235, acc: 0.6673\n","Epoch [32], loss: 0.6141, acc: 0.6794\n","Epoch [33], loss: 0.6184, acc: 0.6770\n","Epoch [34], loss: 0.6188, acc: 0.6709\n","Epoch [35], loss: 0.6139, acc: 0.6745\n","Epoch [36], loss: 0.6147, acc: 0.6733\n","Epoch [37], loss: 0.6088, acc: 0.6818\n","Epoch [38], loss: 0.6127, acc: 0.6806\n","Epoch [39], loss: 0.6121, acc: 0.6794\n","Epoch [40], loss: 0.6104, acc: 0.6818\n","Epoch [41], loss: 0.6117, acc: 0.6697\n","Epoch [42], loss: 0.6117, acc: 0.6770\n","Epoch [43], loss: 0.6084, acc: 0.6709\n","Epoch [44], loss: 0.6049, acc: 0.6818\n","Epoch [45], loss: 0.6070, acc: 0.6818\n","Epoch [46], loss: 0.6045, acc: 0.6952\n","Epoch [47], loss: 0.6017, acc: 0.6842\n","Epoch [48], loss: 0.6017, acc: 0.6903\n","Epoch [49], loss: 0.6018, acc: 0.6879\n","Epoch [50], loss: 0.5997, acc: 0.6939\n","Epoch [51], loss: 0.5982, acc: 0.6879\n","Epoch [52], loss: 0.5981, acc: 0.6842\n","Epoch [53], loss: 0.6000, acc: 0.6855\n","Epoch [54], loss: 0.6030, acc: 0.6806\n","Epoch [55], loss: 0.5977, acc: 0.6915\n","Epoch [56], loss: 0.5953, acc: 0.6739\n","Epoch [57], loss: 0.5952, acc: 0.6770\n","Epoch [58], loss: 0.6063, acc: 0.6818\n","Epoch [59], loss: 0.6036, acc: 0.6867\n","Epoch [60], loss: 0.5973, acc: 0.6818\n","Epoch [61], loss: 0.6022, acc: 0.6733\n","Epoch [62], loss: 0.5957, acc: 0.6842\n","Epoch [63], loss: 0.6014, acc: 0.6915\n","Epoch [64], loss: 0.5979, acc: 0.6830\n","Epoch [65], loss: 0.6070, acc: 0.6745\n","Epoch [66], loss: 0.5991, acc: 0.6770\n","Epoch [67], loss: 0.5979, acc: 0.6842\n","Epoch [68], loss: 0.5958, acc: 0.6927\n","Epoch [69], loss: 0.5992, acc: 0.6733\n","Epoch [70], loss: 0.5964, acc: 0.6842\n","Epoch [71], loss: 0.6026, acc: 0.6879\n","Epoch [72], loss: 0.6014, acc: 0.6939\n","Epoch [73], loss: 0.6056, acc: 0.6879\n","Epoch [74], loss: 0.5954, acc: 0.6915\n","Epoch [75], loss: 0.6061, acc: 0.6745\n","Epoch [76], loss: 0.6071, acc: 0.6745\n","Epoch [77], loss: 0.6032, acc: 0.6903\n","Epoch [78], loss: 0.6078, acc: 0.6709\n","Epoch [79], loss: 0.6022, acc: 0.6915\n","Epoch [80], loss: 0.5971, acc: 0.6988\n","Epoch [81], loss: 0.6000, acc: 0.6903\n","Epoch [82], loss: 0.5969, acc: 0.7024\n","Epoch [83], loss: 0.5998, acc: 0.6903\n","Epoch [84], loss: 0.5970, acc: 0.6939\n","Epoch [85], loss: 0.5995, acc: 0.6976\n","Epoch [86], loss: 0.6019, acc: 0.6903\n","Epoch [87], loss: 0.5992, acc: 0.7024\n","Epoch [88], loss: 0.5995, acc: 0.6855\n","Epoch [89], loss: 0.6024, acc: 0.6855\n","Epoch [90], loss: 0.5951, acc: 0.6848\n","Epoch [91], loss: 0.5993, acc: 0.6903\n","Epoch [92], loss: 0.5961, acc: 0.6915\n","Epoch [93], loss: 0.6024, acc: 0.6745\n","Epoch [94], loss: 0.5968, acc: 0.6927\n","Epoch [95], loss: 0.5988, acc: 0.6903\n","Epoch [96], loss: 0.5936, acc: 0.6770\n","Epoch [97], loss: 0.5971, acc: 0.6770\n","Epoch [98], loss: 0.6025, acc: 0.6842\n","Epoch [99], loss: 0.6002, acc: 0.6982\n"]}]},{"cell_type":"code","source":["feed_forward_df = pd.DataFrame(data = history)\n","accuracy = np.mean(feed_forward_df.acc[50:].to_numpy())\n","loss = np.mean(feed_forward_df.loss[50:].to_numpy())\n","print('For the last 50 inputs, accuracy was {:.4f} and loss was {:.4f}'.format(accuracy,loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8_a_x9MWYHS","executionInfo":{"status":"ok","timestamp":1669758291653,"user_tz":300,"elapsed":132,"user":{"displayName":"Daniel Gaidar","userId":"14786249740454441863"}},"outputId":"0ed0af1e-018b-47e2-831c-ca48a044cb37"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["For the last 50 inputs, accuracy was 0.6832 and loss was 0.6017\n"]}]}]}